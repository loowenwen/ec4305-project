{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48928be5",
   "metadata": {},
   "source": [
    "# HDB Resale Price Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43465c",
   "metadata": {},
   "source": [
    "**Models implemented:**\n",
    "1. Linear Regression (OLS)\n",
    "2. Non-linear Regression (polynomial terms for selected variables)\n",
    "3. IV Regression (2SLS, exploratory)\n",
    "\n",
    "**Common settings:**\n",
    "- 80/20 train-test split\n",
    "- 5-fold Cross-Validation (CV) for hypertuning\n",
    "- RMSE used as main metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44e2db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1e541",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b88751",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/HDB_data_2021_sample.xlsx\"\n",
    "\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "# drop rows with missing resale_price\n",
    "df = df.dropna(subset=[\"resale_price\"])\n",
    "\n",
    "# define target: use log(price) for nicer regression properties\n",
    "df[\"log_resale_price\"] = np.log(df[\"resale_price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda136a",
   "metadata": {},
   "source": [
    "### Feature Selection for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10deaab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for \"full\" models (OLS, LASSO, Ridge, ENet)\n",
    "# we'll use all columns except the target and year (year is constant 2021)\n",
    "drop_cols_full = [\"resale_price\", \"log_resale_price\", \"year\"]\n",
    "X_full = df.drop(columns=drop_cols_full)\n",
    "y = df[\"log_resale_price\"].values\n",
    "\n",
    "feature_names_full = X_full.columns.tolist()\n",
    "\n",
    "# for non-linear regression & stepwise & IV,\n",
    "# we work with a smaller, interpretable subset to keep things manageable.\n",
    "nonlinear_features = [\n",
    "    \"floor_area_sqm\",\n",
    "    \"Remaining_lease\",\n",
    "    \"max_floor_lvl\",\n",
    "    \"mature\",\n",
    "    \"Dist_CBD\",\n",
    "    \"Dist_nearest_station\",\n",
    "    \"Dist_nearest_hospital\",\n",
    "]\n",
    "\n",
    "# keep only those columns that actually exist in the dataframe\n",
    "nonlinear_features = [f for f in nonlinear_features if f in df.columns]\n",
    "\n",
    "X_small = df[nonlinear_features].copy()\n",
    "feature_names_small = X_small.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7c419",
   "metadata": {},
   "source": [
    "## Train-Test Split (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5903bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_train, X_full_test, y_train, y_test = train_test_split(\n",
    "    X_full, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_small_train, X_small_test, y_small_train, y_small_test = train_test_split(\n",
    "    X_small, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d8f0a",
   "metadata": {},
   "source": [
    "## Utility: Compute RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2457de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c470d98",
   "metadata": {},
   "source": [
    "## Linear Regression (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6107afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline OLS - Train RMSE: 0.0732\n",
      "Baseline OLS - Test  RMSE: 0.0772\n"
     ]
    }
   ],
   "source": [
    "# baseline: simple OLS on full feature set\n",
    "ols_baseline = LinearRegression()\n",
    "ols_baseline.fit(X_full_train, y_train)\n",
    "\n",
    "y_pred_train_ols = ols_baseline.predict(X_full_train)\n",
    "y_pred_test_ols = ols_baseline.predict(X_full_test)\n",
    "\n",
    "print(f\"Baseline OLS - Train RMSE: {rmse(y_train, y_pred_train_ols):.4f}\")\n",
    "print(f\"Baseline OLS - Test  RMSE: {rmse(y_test, y_pred_test_ols):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "703664d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS 5-fold CV RMSE (train): 0.0770 (std: 0.0011)\n"
     ]
    }
   ],
   "source": [
    "# \"hypertuned\" OLS:\n",
    "# there's no real hyperparameter to tune for plain OLS,\n",
    "# but we can do a 5-fold CV on the training data to estimate expected RMSE\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_rmse_scores = []\n",
    "for train_idx, val_idx in kf.split(X_full_train):\n",
    "    X_tr, X_val = X_full_train.iloc[train_idx], X_full_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    cv_rmse_scores.append(rmse(y_val, y_val_pred))\n",
    "\n",
    "print(f\"OLS 5-fold CV RMSE (train): {np.mean(cv_rmse_scores):.4f} \"\n",
    "      f\"(std: {np.std(cv_rmse_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdc3b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 OLS coefficients by absolute magnitude:\n",
      "flat_model_terrace                0.626531\n",
      "postal_2digits_50                 0.267955\n",
      "flat_model_2.room                 0.260992\n",
      "flat_model_improved.maisonette    0.237784\n",
      "town_BUKIT.TIMAH                  0.226117\n",
      "postal_2digits_51                 0.209441\n",
      "postal_2digits_18                 0.202498\n",
      "flat_type_2.ROOM                  0.183313\n",
      "postal_2digits_37                 0.173624\n",
      "flat_model_type.s2                0.170621\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# coefficient-based \"feature importance\" for OLS\n",
    "ols_coefs = pd.Series(ols_baseline.coef_, index=feature_names_full)\n",
    "print(\"\\nTop 10 OLS coefficients by absolute magnitude:\")\n",
    "print(ols_coefs.abs().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c78cac",
   "metadata": {},
   "source": [
    "## Non-Linear Regression (Polynomial Terms on Selected Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "033f118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Poly (deg=2) - Train RMSE: 0.1068\n",
      "Baseline Poly (deg=2) - Test  RMSE: 0.1089\n"
     ]
    }
   ],
   "source": [
    "# baseline: degree=2 polynomial on selected features, Linear Regression\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly_train = poly.fit_transform(X_small_train)\n",
    "X_poly_test = poly.transform(X_small_test)\n",
    "\n",
    "poly_feature_names = poly.get_feature_names_out(feature_names_small)\n",
    "\n",
    "poly_ols_baseline = LinearRegression()\n",
    "poly_ols_baseline.fit(X_poly_train, y_small_train)\n",
    "\n",
    "y_pred_train_poly = poly_ols_baseline.predict(X_poly_train)\n",
    "y_pred_test_poly = poly_ols_baseline.predict(X_poly_test)\n",
    "\n",
    "print(f\"Baseline Poly (deg=2) - Train RMSE: {rmse(y_small_train, y_pred_train_poly):.4f}\")\n",
    "print(f\"Baseline Poly (deg=2) - Test  RMSE: {rmse(y_small_test, y_pred_test_poly):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e75b2265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 1 Poly - 5-fold CV RMSE: 0.1205\n",
      "Degree 2 Poly - 5-fold CV RMSE: 0.1079\n",
      "Degree 3 Poly - 5-fold CV RMSE: 0.1022\n",
      "Best polynomial degree (by CV RMSE): 3\n"
     ]
    }
   ],
   "source": [
    "# hypertuned: try degrees 1, 2, 3 with 5-fold CV\n",
    "best_degree = None\n",
    "best_cv_rmse = np.inf\n",
    "\n",
    "for degree in [1, 2, 3]:\n",
    "    poly_tmp = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly_tmp = poly_tmp.fit_transform(X_small_train)\n",
    "\n",
    "    cv_rmse_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_poly_tmp):\n",
    "        X_tr, X_val = X_poly_tmp[train_idx], X_poly_tmp[val_idx]\n",
    "        y_tr, y_val = y_small_train[train_idx], y_small_train[val_idx]\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        cv_rmse_scores.append(rmse(y_val, y_val_pred))\n",
    "\n",
    "    mean_cv_rmse = np.mean(cv_rmse_scores)\n",
    "    print(f\"Degree {degree} Poly - 5-fold CV RMSE: {mean_cv_rmse:.4f}\")\n",
    "\n",
    "    if mean_cv_rmse < best_cv_rmse:\n",
    "        best_cv_rmse = mean_cv_rmse\n",
    "        best_degree = degree\n",
    "\n",
    "print(f\"Best polynomial degree (by CV RMSE): {best_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9c446e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Poly (deg=3) - Test RMSE: 0.1019\n"
     ]
    }
   ],
   "source": [
    "# fit final tuned model with best degree\n",
    "poly_best = PolynomialFeatures(degree=best_degree, include_bias=False)\n",
    "X_poly_train_best = poly_best.fit_transform(X_small_train)\n",
    "X_poly_test_best = poly_best.transform(X_small_test)\n",
    "\n",
    "poly_ols_tuned = LinearRegression()\n",
    "poly_ols_tuned.fit(X_poly_train_best, y_small_train)\n",
    "\n",
    "y_pred_test_poly_tuned = poly_ols_tuned.predict(X_poly_test_best)\n",
    "print(f\"Tuned Poly (deg={best_degree}) - Test RMSE: {rmse(y_small_test, y_pred_test_poly_tuned):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
