{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c7df84",
   "metadata": {},
   "source": [
    "# HDB Resale Price Regression Models: KNN, PCR, and Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a40465",
   "metadata": {},
   "source": [
    "**Models implemented:**\n",
    "1. K-Nearest Neighbors (KNN) Regression\n",
    "2. Principal Component Regression (PCR)\n",
    "3. Neural Network (Multi-layer Perceptron)\n",
    "\n",
    "**Common settings:**\n",
    "- 80/20 train-test split\n",
    "- 5-fold Cross-Validation (CV) for hypertuning\n",
    "- RMSE used as main metric\n",
    "- StandardScaler applied for KNN and NN (distance-based methods require scaling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a68d5a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "✓ Apple GPU (MPS) is available and will be used for neural network training\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Check if MPS (Apple GPU) is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"mps\":\n",
    "    print(\"✓ Apple GPU (MPS) is available and will be used for neural network training\")\n",
    "else:\n",
    "    print(\"⚠ MPS not available, using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40481b57",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f670a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/HDB_data_2021_sample.xlsx\"\n",
    "\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "# drop rows with missing resale_price\n",
    "df = df.dropna(subset=[\"resale_price\"])\n",
    "\n",
    "# define target: use log(price) for nicer regression properties\n",
    "df[\"log_resale_price\"] = np.log(df[\"resale_price\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27220086",
   "metadata": {},
   "source": [
    "### Feature Selection for Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa466946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 228\n"
     ]
    }
   ],
   "source": [
    "# for KNN, PCR, and NN, we'll use all columns except the target and year (year is constant 2021)\n",
    "drop_cols_full = [\"resale_price\", \"log_resale_price\", \"year\"]\n",
    "X_full = df.drop(columns=drop_cols_full)\n",
    "y = df[\"log_resale_price\"].values\n",
    "\n",
    "feature_names_full = X_full.columns.tolist()\n",
    "print(f\"Total number of features: {len(feature_names_full)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcee2ea",
   "metadata": {},
   "source": [
    "## Train-Test Split (80/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac7f001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4800\n",
      "Test set size: 1200\n"
     ]
    }
   ],
   "source": [
    "X_full_train, X_full_test, y_train, y_test = train_test_split(\n",
    "    X_full, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_full_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_full_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f367c7",
   "metadata": {},
   "source": [
    "## Utility: Compute RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b538ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3f623",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN) Regression\n",
    "\n",
    "KNN is a non-parametric method that predicts based on the average of the k nearest neighbors. It requires feature scaling since it uses distance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a4ec99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Baseline - Train RMSE: 0.1081\n",
      "KNN Baseline - Test  RMSE: 0.1423\n"
     ]
    }
   ],
   "source": [
    "# Standardize features for KNN (distance-based method requires scaling)\n",
    "scaler_knn = StandardScaler()\n",
    "X_full_train_scaled_knn = scaler_knn.fit_transform(X_full_train)\n",
    "X_full_test_scaled_knn = scaler_knn.transform(X_full_test)\n",
    "\n",
    "# Baseline: KNN with default parameters (n_neighbors=5)\n",
    "knn_baseline = KNeighborsRegressor()\n",
    "knn_baseline.fit(X_full_train_scaled_knn, y_train)\n",
    "\n",
    "y_pred_train_knn_base = knn_baseline.predict(X_full_train_scaled_knn)\n",
    "y_pred_test_knn_base = knn_baseline.predict(X_full_test_scaled_knn)\n",
    "\n",
    "print(f\"KNN Baseline - Train RMSE: {rmse(y_train, y_pred_train_knn_base):.4f}\")\n",
    "print(f\"KNN Baseline - Test  RMSE: {rmse(y_test, y_pred_test_knn_base):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5468ee1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "\n",
      "KNN Tuned - Best Params: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "KNN Tuned - Test RMSE:  0.1242\n",
      "KNN Tuned - Train RMSE: 0.0041\n"
     ]
    }
   ],
   "source": [
    "# Hypertuned KNN with GridSearchCV\n",
    "# Hyperparameters to tune: n_neighbors, weights, metric\n",
    "knn_param_grid = {\n",
    "    \"n_neighbors\": [3, 5, 7, 10, 15, 20, 25],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"metric\": [\"euclidean\", \"manhattan\"]\n",
    "}\n",
    "\n",
    "knn_grid = GridSearchCV(\n",
    "    estimator=KNeighborsRegressor(),\n",
    "    param_grid=knn_param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "knn_grid.fit(X_full_train_scaled_knn, y_train)\n",
    "\n",
    "best_knn = knn_grid.best_estimator_\n",
    "y_pred_test_knn_tuned = best_knn.predict(X_full_test_scaled_knn)\n",
    "\n",
    "print(f\"\\nKNN Tuned - Best Params: {knn_grid.best_params_}\")\n",
    "print(f\"KNN Tuned - Test RMSE:  {rmse(y_test, y_pred_test_knn_tuned):.4f}\")\n",
    "\n",
    "# Also get train RMSE for comparison\n",
    "y_pred_train_knn_tuned = best_knn.predict(X_full_train_scaled_knn)\n",
    "print(f\"KNN Tuned - Train RMSE: {rmse(y_train, y_pred_train_knn_tuned):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa7633",
   "metadata": {},
   "source": [
    "## Principal Component Regression (PCR)\n",
    "\n",
    "PCR combines Principal Component Analysis (PCA) with linear regression. It reduces dimensionality by projecting features onto principal components, which can help with multicollinearity and overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b86b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCR Baseline (n_components=10) - Train RMSE: 0.2419\n",
      "PCR Baseline (n_components=10) - Test  RMSE: 0.2434\n",
      "\n",
      "Explained variance ratio (first 10 components): 0.2991\n"
     ]
    }
   ],
   "source": [
    "# Standardize features for PCR (PCA is sensitive to scale)\n",
    "scaler_pcr = StandardScaler()\n",
    "X_full_train_scaled_pcr = scaler_pcr.fit_transform(X_full_train)\n",
    "X_full_test_scaled_pcr = scaler_pcr.transform(X_full_test)\n",
    "\n",
    "# Baseline: PCR with a fixed number of components (e.g., 10)\n",
    "# We'll use 10 components as a reasonable baseline\n",
    "n_components_baseline = 10\n",
    "pca_baseline = PCA(n_components=n_components_baseline)\n",
    "X_pca_train_base = pca_baseline.fit_transform(X_full_train_scaled_pcr)\n",
    "X_pca_test_base = pca_baseline.transform(X_full_test_scaled_pcr)\n",
    "\n",
    "pcr_baseline = LinearRegression()\n",
    "pcr_baseline.fit(X_pca_train_base, y_train)\n",
    "\n",
    "y_pred_train_pcr_base = pcr_baseline.predict(X_pca_train_base)\n",
    "y_pred_test_pcr_base = pcr_baseline.predict(X_pca_test_base)\n",
    "\n",
    "print(f\"PCR Baseline (n_components={n_components_baseline}) - Train RMSE: {rmse(y_train, y_pred_train_pcr_base):.4f}\")\n",
    "print(f\"PCR Baseline (n_components={n_components_baseline}) - Test  RMSE: {rmse(y_test, y_pred_test_pcr_base):.4f}\")\n",
    "\n",
    "# Check explained variance\n",
    "explained_var = np.sum(pca_baseline.explained_variance_ratio_)\n",
    "print(f\"\\nExplained variance ratio (first {n_components_baseline} components): {explained_var:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87aa0170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative explained variance by number of components:\n",
      "   5 components: 0.1931\n",
      "  10 components: 0.2991\n",
      "  15 components: 0.3811\n",
      "  20 components: 0.4496\n",
      "  25 components: 0.5106\n",
      "  30 components: 0.5644\n",
      "  50 components: 0.7123\n"
     ]
    }
   ],
   "source": [
    "# First, let's check how many components explain different amounts of variance\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_full_train_scaled_pcr)\n",
    "\n",
    "cumulative_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "print(\"Cumulative explained variance by number of components:\")\n",
    "for n in [5, 10, 15, 20, 25, 30, min(50, len(feature_names_full))]:\n",
    "    if n <= len(cumulative_var):\n",
    "        print(f\"  {n:2d} components: {cumulative_var[n-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97f92857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "\n",
      "PCR Tuned - Best n_components: 50\n",
      "PCR Tuned - Test RMSE:  0.1322\n",
      "PCR Tuned - Train RMSE: 0.1323\n",
      "\n",
      "Explained variance ratio (best 50 components): 0.7123\n"
     ]
    }
   ],
   "source": [
    "# Hypertuned PCR: tune number of components using GridSearchCV\n",
    "# We'll search over a range of component numbers\n",
    "max_components = min(50, X_full_train_scaled_pcr.shape[1], X_full_train_scaled_pcr.shape[0] - 1)\n",
    "component_range = list(range(5, max_components + 1, 5))  # Try 5, 10, 15, ..., up to max\n",
    "\n",
    "# Create a pipeline for PCR\n",
    "pcr_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA()),\n",
    "    (\"regression\", LinearRegression())\n",
    "])\n",
    "\n",
    "pcr_param_grid = {\n",
    "    \"pca__n_components\": component_range\n",
    "}\n",
    "\n",
    "pcr_grid = GridSearchCV(\n",
    "    estimator=pcr_pipeline,\n",
    "    param_grid=pcr_param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "pcr_grid.fit(X_full_train, y_train)\n",
    "\n",
    "best_pcr = pcr_grid.best_estimator_\n",
    "y_pred_test_pcr_tuned = best_pcr.predict(X_full_test)\n",
    "\n",
    "print(f\"\\nPCR Tuned - Best n_components: {pcr_grid.best_params_['pca__n_components']}\")\n",
    "print(f\"PCR Tuned - Test RMSE:  {rmse(y_test, y_pred_test_pcr_tuned):.4f}\")\n",
    "\n",
    "# Also get train RMSE for comparison\n",
    "y_pred_train_pcr_tuned = best_pcr.predict(X_full_train)\n",
    "print(f\"PCR Tuned - Train RMSE: {rmse(y_train, y_pred_train_pcr_tuned):.4f}\")\n",
    "\n",
    "# Get explained variance for best model\n",
    "best_n_components = pcr_grid.best_params_['pca__n_components']\n",
    "pca_best = best_pcr.named_steps['pca']\n",
    "explained_var_best = np.sum(pca_best.explained_variance_ratio_)\n",
    "print(f\"\\nExplained variance ratio (best {best_n_components} components): {explained_var_best:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24a665",
   "metadata": {},
   "source": [
    "## Neural Network (Multi-layer Perceptron)\n",
    "\n",
    "Neural networks can capture complex non-linear relationships. We'll use sklearn's MLPRegressor, which is a feedforward neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7560aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch neural network model and training functions\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=(100,), activation='relu', alpha=0.0001):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        \n",
    "        # Build layers\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            if activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'leaky_relu':\n",
    "                layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "            elif activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer (single value for regression)\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.alpha = alpha  # L2 regularization (weight decay)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "# Training function\n",
    "def train_pytorch_nn(X_train, y_train, X_val, y_val, hidden_sizes=(100,), \n",
    "                     activation='relu', alpha=0.001, learning_rate=0.001,\n",
    "                     epochs=300, batch_size=64, patience=30, device=device):\n",
    "    \"\"\"\n",
    "    Train a PyTorch neural network with early stopping.\n",
    "    Returns: trained model, training history\n",
    "    \"\"\"\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).to(device)\n",
    "    \n",
    "    # Create model\n",
    "    model = MLPRegressor(\n",
    "        input_size=X_train.shape[1],\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        activation=activation,\n",
    "        alpha=alpha\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss and optimizer (Adam with weight decay for L2 regularization)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=alpha)\n",
    "    \n",
    "    # Training with early stopping\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), \n",
    "                             batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(y_val_pred, y_val_tensor).item()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, {'train_losses': train_losses, 'val_losses': val_losses}\n",
    "\n",
    "# Prediction function\n",
    "def predict_pytorch_nn(model, X, device=device):\n",
    "    \"\"\"Make predictions with PyTorch model\"\"\"\n",
    "    model.eval()\n",
    "    X_tensor = torch.FloatTensor(X).to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_tensor).cpu().numpy()\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19f55028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline neural network...\n",
      "Parameters: hidden_sizes=(100,), activation='relu', lr=0.001\n",
      "\n",
      "NN Baseline - Train RMSE: 0.1704\n",
      "NN Baseline - Test  RMSE: 0.2033\n",
      "NN Baseline - Training epochs: 159\n",
      "NN Baseline - Final train loss: 0.034670\n",
      "NN Baseline - Final val loss: 0.083216\n"
     ]
    }
   ],
   "source": [
    "# Standardize features for Neural Network (required for convergence)\n",
    "scaler_nn = StandardScaler()\n",
    "X_full_train_scaled_nn = scaler_nn.fit_transform(X_full_train)\n",
    "X_full_test_scaled_nn = scaler_nn.transform(X_full_test)\n",
    "\n",
    "# Baseline: Simple MLP with default parameters\n",
    "print(\"Training baseline neural network...\")\n",
    "print(\"Parameters: hidden_sizes=(100,), activation='relu', lr=0.001\")\n",
    "\n",
    "# Split training data for validation (10% for early stopping)\n",
    "X_nn_train, X_nn_val, y_nn_train, y_nn_val = train_test_split(\n",
    "    X_full_train_scaled_nn, y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "nn_baseline, baseline_history = train_pytorch_nn(\n",
    "    X_nn_train, y_nn_train, X_nn_val, y_nn_val,\n",
    "    hidden_sizes=(100,),\n",
    "    activation='relu',\n",
    "    learning_rate=0.001,\n",
    "    epochs=300,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "y_pred_train_nn_base = predict_pytorch_nn(nn_baseline, X_full_train_scaled_nn, device)\n",
    "y_pred_test_nn_base = predict_pytorch_nn(nn_baseline, X_full_test_scaled_nn, device)\n",
    "\n",
    "print(f\"\\nNN Baseline - Train RMSE: {rmse(y_train, y_pred_train_nn_base):.4f}\")\n",
    "print(f\"NN Baseline - Test  RMSE: {rmse(y_test, y_pred_test_nn_base):.4f}\")\n",
    "print(f\"NN Baseline - Training epochs: {len(baseline_history['train_losses'])}\")\n",
    "print(f\"NN Baseline - Final train loss: {baseline_history['train_losses'][-1]:.6f}\")\n",
    "print(f\"NN Baseline - Final val loss: {baseline_history['val_losses'][-1]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f786ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning with 5-fold CV...\n",
      "Testing 36 parameter combinations\n",
      "\n",
      "Testing 1/36: {'hidden_sizes': (50,), 'activation': 'relu', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.2163\n",
      "\n",
      "Testing 2/36: {'hidden_sizes': (50,), 'activation': 'relu', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 0.2881\n",
      "\n",
      "Testing 3/36: {'hidden_sizes': (50,), 'activation': 'relu', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 31.7597\n",
      "\n",
      "Testing 4/36: {'hidden_sizes': (50,), 'activation': 'leaky_relu', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.2045\n",
      "\n",
      "Testing 5/36: {'hidden_sizes': (50,), 'activation': 'leaky_relu', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 0.3384\n",
      "\n",
      "Testing 6/36: {'hidden_sizes': (50,), 'activation': 'leaky_relu', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 71.1036\n",
      "\n",
      "Testing 7/36: {'hidden_sizes': (50,), 'activation': 'tanh', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.3976\n",
      "\n",
      "Testing 8/36: {'hidden_sizes': (50,), 'activation': 'tanh', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 0.3716\n",
      "\n",
      "Testing 9/36: {'hidden_sizes': (50,), 'activation': 'tanh', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 0.2049\n",
      "\n",
      "Testing 10/36: {'hidden_sizes': (100,), 'activation': 'relu', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.2085\n",
      "\n",
      "Testing 11/36: {'hidden_sizes': (100,), 'activation': 'relu', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 0.3774\n",
      "\n",
      "Testing 12/36: {'hidden_sizes': (100,), 'activation': 'relu', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 65.0246\n",
      "\n",
      "Testing 13/36: {'hidden_sizes': (100,), 'activation': 'leaky_relu', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.2061\n",
      "\n",
      "Testing 14/36: {'hidden_sizes': (100,), 'activation': 'leaky_relu', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 1.4130\n",
      "\n",
      "Testing 15/36: {'hidden_sizes': (100,), 'activation': 'leaky_relu', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 35.8707\n",
      "\n",
      "Testing 16/36: {'hidden_sizes': (100,), 'activation': 'tanh', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.3738\n",
      "\n",
      "Testing 17/36: {'hidden_sizes': (100,), 'activation': 'tanh', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 0.3300\n",
      "\n",
      "Testing 18/36: {'hidden_sizes': (100,), 'activation': 'tanh', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 0.3850\n",
      "\n",
      "Testing 19/36: {'hidden_sizes': (150,), 'activation': 'relu', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.2077\n",
      "\n",
      "Testing 20/36: {'hidden_sizes': (150,), 'activation': 'relu', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 0.3530\n",
      "\n",
      "Testing 21/36: {'hidden_sizes': (150,), 'activation': 'relu', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 73.4876\n",
      "\n",
      "Testing 22/36: {'hidden_sizes': (150,), 'activation': 'leaky_relu', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.2376\n",
      "\n",
      "Testing 23/36: {'hidden_sizes': (150,), 'activation': 'leaky_relu', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 0.5990\n",
      "\n",
      "Testing 24/36: {'hidden_sizes': (150,), 'activation': 'leaky_relu', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 167.1990\n",
      "\n",
      "Testing 25/36: {'hidden_sizes': (150,), 'activation': 'tanh', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.3769\n",
      "\n",
      "Testing 26/36: {'hidden_sizes': (150,), 'activation': 'tanh', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 0.3307\n",
      "\n",
      "Testing 27/36: {'hidden_sizes': (150,), 'activation': 'tanh', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 0.4918\n",
      "\n",
      "Testing 28/36: {'hidden_sizes': (100, 50), 'activation': 'relu', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.2252\n",
      "\n",
      "Testing 29/36: {'hidden_sizes': (100, 50), 'activation': 'relu', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 0.3102\n",
      "\n",
      "Testing 30/36: {'hidden_sizes': (100, 50), 'activation': 'relu', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 0.2997\n",
      "\n",
      "Testing 31/36: {'hidden_sizes': (100, 50), 'activation': 'leaky_relu', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.2050\n",
      "\n",
      "Testing 32/36: {'hidden_sizes': (100, 50), 'activation': 'leaky_relu', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 0.2808\n",
      "\n",
      "Testing 33/36: {'hidden_sizes': (100, 50), 'activation': 'leaky_relu', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 7.3626\n",
      "\n",
      "Testing 34/36: {'hidden_sizes': (100, 50), 'activation': 'tanh', 'learning_rate': 0.001}\n",
      "  Mean CV RMSE: 0.2141\n",
      "\n",
      "Testing 35/36: {'hidden_sizes': (100, 50), 'activation': 'tanh', 'learning_rate': 0.01}\n",
      "  Mean CV RMSE: 0.2566\n",
      "\n",
      "Testing 36/36: {'hidden_sizes': (100, 50), 'activation': 'tanh', 'learning_rate': 0.1}\n",
      "  Mean CV RMSE: 0.1127\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING COMPLETE\n",
      "============================================================\n",
      "Best parameters: {'hidden_sizes': (100, 50), 'activation': 'tanh', 'learning_rate': 0.1}\n",
      "Best CV RMSE: 0.1127\n",
      "\n",
      "NN Tuned - Test RMSE:  0.1620\n",
      "NN Tuned - Train RMSE: 0.1537\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting hyperparameter tuning with 5-fold CV...\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "param_combinations = []\n",
    "for hidden_sizes in [(50,), (100,), (150,), (100, 50)]:\n",
    "    for activation in [\"relu\", \"leaky_relu\", \"tanh\"]:\n",
    "          for lr in [0.001, 0.01, 0.1]:\n",
    "              param_combinations.append(\n",
    "                  {\n",
    "                      \"hidden_sizes\": hidden_sizes,\n",
    "                      \"activation\": activation,\n",
    "                      \"learning_rate\": lr,\n",
    "                  }\n",
    "              )\n",
    "\n",
    "print(f\"Testing {len(param_combinations)} parameter combinations\\n\")\n",
    "\n",
    "best_score = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "# Grid search with cross-validation\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f\"Testing {i+1}/{len(param_combinations)}: {params}\")\n",
    "\n",
    "    cv_rmse_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_full_train_scaled_nn)):\n",
    "        X_tr, X_val = X_full_train_scaled_nn[train_idx], X_full_train_scaled_nn[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        # Use the CV validation fold for early stopping\n",
    "        model, _ = train_pytorch_nn(\n",
    "            X_tr,\n",
    "            y_tr,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            hidden_sizes=params[\"hidden_sizes\"],\n",
    "            activation=params[\"activation\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            epochs=500,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # Evaluate on the CV validation fold\n",
    "        y_val_pred = predict_pytorch_nn(model, X_val, device)\n",
    "        cv_rmse_scores.append(rmse(y_val, y_val_pred))\n",
    "\n",
    "    mean_cv_rmse = np.mean(cv_rmse_scores)\n",
    "    print(f\"  Mean CV RMSE: {mean_cv_rmse:.4f}\\n\")\n",
    "\n",
    "    if mean_cv_rmse < best_score:\n",
    "        best_score = mean_cv_rmse\n",
    "        best_params = params.copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best CV RMSE: {best_score:.4f}\")\n",
    "\n",
    "# Retrain on FULL training set with best hyperparameters\n",
    "# Split training data for validation (10% for early stopping)\n",
    "X_tr_inner, X_val_inner, y_tr_inner, y_val_inner = train_test_split(\n",
    "    X_full_train_scaled_nn, y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "final_model, _ = train_pytorch_nn(\n",
    "    X_tr_inner,\n",
    "    y_tr_inner,\n",
    "    X_val_inner,\n",
    "    y_val_inner,\n",
    "    hidden_sizes=best_params[\"hidden_sizes\"],\n",
    "    activation=best_params[\"activation\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    epochs=500,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Evaluate final model\n",
    "y_pred_test_nn_tuned = predict_pytorch_nn(final_model, X_full_test_scaled_nn, device)\n",
    "y_pred_train_nn_tuned = predict_pytorch_nn(final_model, X_full_train_scaled_nn, device)\n",
    "\n",
    "print(f\"\\nNN Tuned - Test RMSE:  {rmse(y_test, y_pred_test_nn_tuned):.4f}\")\n",
    "print(f\"NN Tuned - Train RMSE: {rmse(y_train, y_pred_train_nn_tuned):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2533d3f",
   "metadata": {},
   "source": [
    "## Model Comparison Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d9977f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "============================================================\n",
      "       Model  Train RMSE  Test RMSE\n",
      "KNN Baseline    0.108149   0.142297\n",
      "   KNN Tuned    0.004133   0.124174\n",
      "PCR Baseline    0.241927   0.243392\n",
      "   PCR Tuned    0.132263   0.132157\n",
      " NN Baseline    0.170382   0.203317\n",
      "    NN Tuned    0.153715   0.162027\n",
      "============================================================\n",
      "\n",
      "Best model (by Test RMSE): KNN Tuned\n",
      "Best Test RMSE: 0.1242\n"
     ]
    }
   ],
   "source": [
    "# Create a summary comparison table\n",
    "results_summary = pd.DataFrame({\n",
    "    \"Model\": [\"KNN Baseline\", \"KNN Tuned\", \n",
    "              \"PCR Baseline\", \"PCR Tuned\",\n",
    "              \"NN Baseline\", \"NN Tuned\"],\n",
    "    \"Train RMSE\": [\n",
    "        rmse(y_train, y_pred_train_knn_base),\n",
    "        rmse(y_train, y_pred_train_knn_tuned),\n",
    "        rmse(y_train, y_pred_train_pcr_base),\n",
    "        rmse(y_train, y_pred_train_pcr_tuned),\n",
    "        rmse(y_train, y_pred_train_nn_base),\n",
    "        rmse(y_train, y_pred_train_nn_tuned)\n",
    "    ],\n",
    "    \"Test RMSE\": [\n",
    "        rmse(y_test, y_pred_test_knn_base),\n",
    "        rmse(y_test, y_pred_test_knn_tuned),\n",
    "        rmse(y_test, y_pred_test_pcr_base),\n",
    "        rmse(y_test, y_pred_test_pcr_tuned),\n",
    "        rmse(y_test, y_pred_test_nn_base),\n",
    "        rmse(y_test, y_pred_test_nn_tuned)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(results_summary.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find best model by test RMSE\n",
    "best_test_rmse_idx = results_summary[\"Test RMSE\"].idxmin()\n",
    "best_model_name = results_summary.loc[best_test_rmse_idx, \"Model\"]\n",
    "best_test_rmse = results_summary.loc[best_test_rmse_idx, \"Test RMSE\"]\n",
    "\n",
    "print(f\"\\nBest model (by Test RMSE): {best_model_name}\")\n",
    "print(f\"Best Test RMSE: {best_test_rmse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BlackjackAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
